{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "guitar_brand_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "f06jbnuGjA6v",
        "colab_type": "code",
        "outputId": "ed13d7c2-dcb8-4e35-a876-ca62ba013d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install icrawler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting icrawler\n",
            "  Downloading https://files.pythonhosted.org/packages/78/a2/1ac26a2c39b87bef4ef8cb39cb4f33e04041f4a9f04f8cc3dfa1251e0304/icrawler-0.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from icrawler) (2.18.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from icrawler) (4.2.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from icrawler) (4.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from icrawler) (1.11.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from icrawler) (4.6.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->icrawler) (2018.11.29)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->icrawler) (0.46)\n",
            "Installing collected packages: icrawler\n",
            "Successfully installed icrawler-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mNuVJ10DjE5P",
        "colab_type": "code",
        "outputId": "2e69bd0a-7871-47df-f5e8-8ac5df2c84e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HwrMWxPYjfx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "\n",
        "brands = ['Fender', 'Gibson', 'Ibanez', 'Caparison', 'Jackson', \n",
        "          'DEAN', 'B.C. Rich', 'ESP', 'SCHECTER', 'Paul Reed Smith', \n",
        "          'Mayones','Strandberg', 'Kiesel', 'Suhr']\n",
        "\n",
        "for brand in brands:\n",
        "  os.makedirs('./data/{}'.format(brand), exist_ok=True)\n",
        "  crawler = GoogleImageCrawler(storage={\"root_dir\": \"./data/{}\".format(brand)})\n",
        "  crawler.crawl(keyword=\"{} guitar\".format(brand), max_num=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2bjrQZzjW_Z",
        "colab_type": "code",
        "outputId": "876dc1d2-d00f-4faa-d5b4-fe61e71a93a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/My Drive/Colab Notebooks/personal_project/guitar_brand_recognition"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/personal_project/guitar_brand_recognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LgkUz2iklZ6Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "test_size = 0.2\n",
        "\n",
        "p = Path('.')\n",
        "\n",
        "for brand in brands:\n",
        "    # 元画像ディレクトリのパスオブジェクト\n",
        "    raw_dir = p / 'data' / brand\n",
        "    \n",
        "    # 訓練データを格納するディレクトリ作成\n",
        "    train_set = p / 'train_set' / brand\n",
        "    train_set.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # テストデータを格納するディレクトリ作成\n",
        "    test_set = p / 'test_set' / brand\n",
        "    test_set.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "     # 各ブランドのの画像一覧取得\n",
        "    imgs_path = list(raw_dir.iterdir())\n",
        "    \n",
        "    # 訓練データを格納\n",
        "    for img in imgs_path[:int(len(imgs_path)*0.2)]:\n",
        "        # 訓練画像をコピー\n",
        "        shutil.copy(str(img), str(test_set.resolve()))\n",
        "        \n",
        "    # テストデータを格納\n",
        "    for img in imgs_path[int(len(imgs_path)*0.2):]:\n",
        "        # 訓練画像をコピー\n",
        "        shutil.copy(str(img), str(train_set.resolve()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WECq1cohukA7",
        "colab_type": "code",
        "outputId": "d07e1897-55ef-4fc8-c935-1260140cdf05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "path = Path('./train_set')\n",
        "\n",
        "num_imgs = 0\n",
        "for brand in path.iterdir():\n",
        "  for img in brand.iterdir():\n",
        "    num_imgs += 1\n",
        "    \n",
        "num_imgs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "4IpwLvp2KjL5",
        "colab_type": "code",
        "outputId": "62d489d1-7352-4cd3-b9a3-139697d50c21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "path = Path('./test_set')\n",
        "\n",
        "num_imgs = 0\n",
        "for brand in path.iterdir():\n",
        "  for img in brand.iterdir():\n",
        "    num_imgs += 1\n",
        "    \n",
        "num_imgs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1046"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "4hUpGPJ7pX68",
        "colab_type": "code",
        "outputId": "b77576f9-1241-4990-f47a-fa809c6d4245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1193
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.callbacks import EarlyStopping \n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NUM_TRAINING = 5005\n",
        "NUM_VALIDATION = 1046\n",
        "\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "inceptionv3_model =InceptionV3(weights='imagenet', include_top=False, \n",
        "                         input_tensor=input_tensor)\n",
        "\n",
        "x = inceptionv3_model.output\n",
        "x = Flatten()(x)\n",
        "predictions = Dense(14, activation='softmax',kernel_regularizer=regularizers.l2(0.01), name='predictions')(x)\n",
        "model = Model(inputs=inceptionv3_model.input, outputs=predictions)\n",
        "\n",
        "for layer in inceptionv3_model.layers[:249]:\n",
        "  layer.trainable = False\n",
        "  \n",
        "  if layer.name.startswith('batch_normalization'):\n",
        "        layer.trainable = True\n",
        "\n",
        "for layer in inceptionv3_model.layers[249:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'train_set',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "   'test_set',\n",
        "   target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "   batch_size=BATCH_SIZE,\n",
        "   class_mode='categorical',\n",
        "   shuffle=True\n",
        ")\n",
        "\n",
        "hist = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=NUM_TRAINING // BATCH_SIZE,\n",
        "    epochs=32,\n",
        "    verbose=1,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=NUM_VALIDATION//BATCH_SIZE\n",
        ")\n",
        "\n",
        "model.save('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4992 images belonging to 14 classes.\n",
            "Found 1044 images belonging to 14 classes.\n",
            "Epoch 1/32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:872: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 213s 1s/step - loss: 3.7225 - acc: 0.2111 - val_loss: 2.5769 - val_acc: 0.3506\n",
            "Epoch 2/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 2.2404 - acc: 0.3584 - val_loss: 1.8086 - val_acc: 0.4990\n",
            "Epoch 3/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 1.8689 - acc: 0.4535 - val_loss: 1.7391 - val_acc: 0.5909\n",
            "Epoch 4/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 1.6368 - acc: 0.5172 - val_loss: 1.3490 - val_acc: 0.6443\n",
            "Epoch 5/32\n",
            "156/156 [==============================] - 182s 1s/step - loss: 1.4292 - acc: 0.5783 - val_loss: 1.1855 - val_acc: 0.6848\n",
            "Epoch 6/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 1.2882 - acc: 0.6264 - val_loss: 1.1275 - val_acc: 0.6966\n",
            "Epoch 7/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 1.1925 - acc: 0.6595 - val_loss: 0.9743 - val_acc: 0.7441\n",
            "Epoch 8/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 1.0330 - acc: 0.7057 - val_loss: 0.8192 - val_acc: 0.7935\n",
            "Epoch 9/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.9360 - acc: 0.7414 - val_loss: 0.7791 - val_acc: 0.8202\n",
            "Epoch 10/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 0.8224 - acc: 0.7815 - val_loss: 0.7407 - val_acc: 0.8221\n",
            "Epoch 11/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.7439 - acc: 0.7993 - val_loss: 0.7926 - val_acc: 0.8458\n",
            "Epoch 12/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 0.6880 - acc: 0.8245 - val_loss: 0.7194 - val_acc: 0.8646\n",
            "Epoch 13/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 0.6424 - acc: 0.8271 - val_loss: 0.6239 - val_acc: 0.8715\n",
            "Epoch 14/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.5867 - acc: 0.8425 - val_loss: 0.5567 - val_acc: 0.8804\n",
            "Epoch 15/32\n",
            "156/156 [==============================] - 182s 1s/step - loss: 0.5442 - acc: 0.8550 - val_loss: 0.7633 - val_acc: 0.8547\n",
            "Epoch 16/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 0.5143 - acc: 0.8716 - val_loss: 0.6461 - val_acc: 0.8735\n",
            "Epoch 17/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.4817 - acc: 0.8750 - val_loss: 0.7086 - val_acc: 0.8626\n",
            "Epoch 18/32\n",
            "156/156 [==============================] - 185s 1s/step - loss: 0.4469 - acc: 0.8908 - val_loss: 0.7712 - val_acc: 0.8706\n",
            "Epoch 19/32\n",
            "156/156 [==============================] - 187s 1s/step - loss: 0.4167 - acc: 0.9040 - val_loss: 0.5429 - val_acc: 0.9051\n",
            "Epoch 20/32\n",
            "156/156 [==============================] - 185s 1s/step - loss: 0.4293 - acc: 0.8934 - val_loss: 0.6512 - val_acc: 0.8844\n",
            "Epoch 21/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 0.3760 - acc: 0.9129 - val_loss: 0.6979 - val_acc: 0.8745\n",
            "Epoch 22/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.3480 - acc: 0.9193 - val_loss: 0.6006 - val_acc: 0.8913\n",
            "Epoch 23/32\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.3350 - acc: 0.9179 - val_loss: 0.5075 - val_acc: 0.9042\n",
            "Epoch 24/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 0.3519 - acc: 0.9201 - val_loss: 0.7841 - val_acc: 0.8686\n",
            "Epoch 25/32\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.3445 - acc: 0.9217 - val_loss: 0.4966 - val_acc: 0.9061\n",
            "Epoch 26/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.3097 - acc: 0.9333 - val_loss: 0.5857 - val_acc: 0.9022\n",
            "Epoch 27/32\n",
            "156/156 [==============================] - 183s 1s/step - loss: 0.2779 - acc: 0.9365 - val_loss: 0.4711 - val_acc: 0.9348\n",
            "Epoch 28/32\n",
            "156/156 [==============================] - 185s 1s/step - loss: 0.2814 - acc: 0.9385 - val_loss: 0.6312 - val_acc: 0.8794\n",
            "Epoch 29/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.3046 - acc: 0.9305 - val_loss: 0.5334 - val_acc: 0.9051\n",
            "Epoch 30/32\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.2616 - acc: 0.9443 - val_loss: 0.5849 - val_acc: 0.9002\n",
            "Epoch 31/32\n",
            "156/156 [==============================] - 185s 1s/step - loss: 0.3041 - acc: 0.9325 - val_loss: 0.5656 - val_acc: 0.8972\n",
            "Epoch 32/32\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.2431 - acc: 0.9495 - val_loss: 0.5036 - val_acc: 0.9091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UhSkmzvdxhDq",
        "colab_type": "code",
        "outputId": "576116ee-98bd-40a0-a3d7-a3ad950150c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2261
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.callbacks import EarlyStopping \n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NUM_TRAINING = 5005\n",
        "NUM_VALIDATION = 1046\n",
        "\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "inceptionv3_model =InceptionV3(weights='imagenet', include_top=False, \n",
        "                         input_tensor=input_tensor)\n",
        "\n",
        "x = inceptionv3_model.output\n",
        "x = Flatten()(x)\n",
        "predictions = Dense(14, activation='softmax',kernel_regularizer=regularizers.l2(0.01), name='predictions')(x)\n",
        "model = Model(inputs=inceptionv3_model.input, outputs=predictions)\n",
        "\n",
        "for layer in inceptionv3_model.layers[:249]:\n",
        "  layer.trainable = False\n",
        "  \n",
        "  if layer.name.startswith('batch_normalization'):\n",
        "        layer.trainable = True\n",
        "\n",
        "for layer in inceptionv3_model.layers[249:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'train_set',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "   'test_set',\n",
        "   target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "   batch_size=BATCH_SIZE,\n",
        "   class_mode='categorical',\n",
        "   shuffle=True\n",
        ")\n",
        "\n",
        "hist = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=NUM_TRAINING // BATCH_SIZE,\n",
        "    epochs=64,\n",
        "    verbose=1,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=NUM_VALIDATION//BATCH_SIZE\n",
        ")\n",
        "\n",
        "model.save('model_1.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4992 images belonging to 14 classes.\n",
            "Found 1044 images belonging to 14 classes.\n",
            "Epoch 1/64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:872: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 197s 1s/step - loss: 3.9917 - acc: 0.2252 - val_loss: 3.2390 - val_acc: 0.3613\n",
            "Epoch 2/64\n",
            "156/156 [==============================] - 176s 1s/step - loss: 2.3971 - acc: 0.3518 - val_loss: 1.6853 - val_acc: 0.5435\n",
            "Epoch 3/64\n",
            "156/156 [==============================] - 171s 1s/step - loss: 1.8947 - acc: 0.4437 - val_loss: 1.5899 - val_acc: 0.5623\n",
            "Epoch 4/64\n",
            "156/156 [==============================] - 172s 1s/step - loss: 1.7549 - acc: 0.4900 - val_loss: 1.3290 - val_acc: 0.6156\n",
            "Epoch 5/64\n",
            "156/156 [==============================] - 172s 1s/step - loss: 1.4825 - acc: 0.5647 - val_loss: 1.2097 - val_acc: 0.6571\n",
            "Epoch 6/64\n",
            "156/156 [==============================] - 173s 1s/step - loss: 1.3010 - acc: 0.6206 - val_loss: 1.0468 - val_acc: 0.6897\n",
            "Epoch 7/64\n",
            "156/156 [==============================] - 171s 1s/step - loss: 1.1756 - acc: 0.6587 - val_loss: 0.8440 - val_acc: 0.7796\n",
            "Epoch 8/64\n",
            "156/156 [==============================] - 171s 1s/step - loss: 1.0425 - acc: 0.6965 - val_loss: 0.8109 - val_acc: 0.8083\n",
            "Epoch 9/64\n",
            "156/156 [==============================] - 170s 1s/step - loss: 0.9313 - acc: 0.7374 - val_loss: 1.0805 - val_acc: 0.7451\n",
            "Epoch 10/64\n",
            "156/156 [==============================] - 169s 1s/step - loss: 0.8587 - acc: 0.7636 - val_loss: 0.9736 - val_acc: 0.7619\n",
            "Epoch 11/64\n",
            "156/156 [==============================] - 171s 1s/step - loss: 0.7655 - acc: 0.7883 - val_loss: 0.9304 - val_acc: 0.8113\n",
            "Epoch 12/64\n",
            "156/156 [==============================] - 167s 1s/step - loss: 0.6990 - acc: 0.8095 - val_loss: 0.7643 - val_acc: 0.8360\n",
            "Epoch 13/64\n",
            "156/156 [==============================] - 169s 1s/step - loss: 0.6603 - acc: 0.8227 - val_loss: 0.7802 - val_acc: 0.8271\n",
            "Epoch 14/64\n",
            "156/156 [==============================] - 169s 1s/step - loss: 0.5643 - acc: 0.8496 - val_loss: 0.5789 - val_acc: 0.8696\n",
            "Epoch 15/64\n",
            "156/156 [==============================] - 170s 1s/step - loss: 0.5697 - acc: 0.8520 - val_loss: 0.5961 - val_acc: 0.8834\n",
            "Epoch 16/64\n",
            "156/156 [==============================] - 170s 1s/step - loss: 0.5040 - acc: 0.8708 - val_loss: 0.5968 - val_acc: 0.8804\n",
            "Epoch 17/64\n",
            "156/156 [==============================] - 169s 1s/step - loss: 0.4767 - acc: 0.8780 - val_loss: 0.5779 - val_acc: 0.8893\n",
            "Epoch 18/64\n",
            "156/156 [==============================] - 166s 1s/step - loss: 0.4439 - acc: 0.8924 - val_loss: 0.6501 - val_acc: 0.8715\n",
            "Epoch 19/64\n",
            "156/156 [==============================] - 169s 1s/step - loss: 0.4485 - acc: 0.8850 - val_loss: 0.6515 - val_acc: 0.8735\n",
            "Epoch 20/64\n",
            "156/156 [==============================] - 170s 1s/step - loss: 0.3963 - acc: 0.9012 - val_loss: 0.8028 - val_acc: 0.8686\n",
            "Epoch 21/64\n",
            "156/156 [==============================] - 167s 1s/step - loss: 0.3985 - acc: 0.9006 - val_loss: 0.5521 - val_acc: 0.9051\n",
            "Epoch 22/64\n",
            "156/156 [==============================] - 168s 1s/step - loss: 0.3668 - acc: 0.9155 - val_loss: 0.6065 - val_acc: 0.9042\n",
            "Epoch 23/64\n",
            "156/156 [==============================] - 178s 1s/step - loss: 0.3784 - acc: 0.9105 - val_loss: 0.6496 - val_acc: 0.8854\n",
            "Epoch 24/64\n",
            "156/156 [==============================] - 187s 1s/step - loss: 0.3597 - acc: 0.9131 - val_loss: 0.4990 - val_acc: 0.9061\n",
            "Epoch 25/64\n",
            "156/156 [==============================] - 187s 1s/step - loss: 0.3076 - acc: 0.9319 - val_loss: 0.5746 - val_acc: 0.8913\n",
            "Epoch 26/64\n",
            "156/156 [==============================] - 188s 1s/step - loss: 0.3114 - acc: 0.9301 - val_loss: 0.5470 - val_acc: 0.9002\n",
            "Epoch 27/64\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.3112 - acc: 0.9323 - val_loss: 0.6131 - val_acc: 0.8874\n",
            "Epoch 28/64\n",
            "156/156 [==============================] - 189s 1s/step - loss: 0.3077 - acc: 0.9333 - val_loss: 0.4925 - val_acc: 0.9061\n",
            "Epoch 29/64\n",
            "156/156 [==============================] - 188s 1s/step - loss: 0.2780 - acc: 0.9403 - val_loss: 0.5488 - val_acc: 0.8992\n",
            "Epoch 30/64\n",
            "156/156 [==============================] - 188s 1s/step - loss: 0.2895 - acc: 0.9337 - val_loss: 0.4904 - val_acc: 0.9239\n",
            "Epoch 31/64\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.2749 - acc: 0.9401 - val_loss: 0.4963 - val_acc: 0.9190\n",
            "Epoch 32/64\n",
            "156/156 [==============================] - 182s 1s/step - loss: 0.3290 - acc: 0.9327 - val_loss: 1.0300 - val_acc: 0.8656\n",
            "Epoch 33/64\n",
            "156/156 [==============================] - 180s 1s/step - loss: 0.2965 - acc: 0.9363 - val_loss: 0.6219 - val_acc: 0.8814\n",
            "Epoch 34/64\n",
            "156/156 [==============================] - 179s 1s/step - loss: 0.2549 - acc: 0.9459 - val_loss: 0.5161 - val_acc: 0.9072\n",
            "Epoch 35/64\n",
            "156/156 [==============================] - 177s 1s/step - loss: 0.2522 - acc: 0.9501 - val_loss: 0.4704 - val_acc: 0.9200\n",
            "Epoch 36/64\n",
            "156/156 [==============================] - 177s 1s/step - loss: 0.2401 - acc: 0.9509 - val_loss: 0.4833 - val_acc: 0.9140\n",
            "Epoch 37/64\n",
            "156/156 [==============================] - 176s 1s/step - loss: 0.2232 - acc: 0.9573 - val_loss: 0.5404 - val_acc: 0.9140\n",
            "Epoch 38/64\n",
            "156/156 [==============================] - 178s 1s/step - loss: 0.2431 - acc: 0.9471 - val_loss: 0.4804 - val_acc: 0.9170\n",
            "Epoch 39/64\n",
            "156/156 [==============================] - 177s 1s/step - loss: 0.2314 - acc: 0.9513 - val_loss: 0.5083 - val_acc: 0.9130\n",
            "Epoch 40/64\n",
            "156/156 [==============================] - 177s 1s/step - loss: 0.2120 - acc: 0.9565 - val_loss: 0.4092 - val_acc: 0.9239\n",
            "Epoch 41/64\n",
            "156/156 [==============================] - 175s 1s/step - loss: 0.2482 - acc: 0.9483 - val_loss: 0.5304 - val_acc: 0.9111\n",
            "Epoch 42/64\n",
            "156/156 [==============================] - 176s 1s/step - loss: 0.2158 - acc: 0.9579 - val_loss: 0.4566 - val_acc: 0.9081\n",
            "Epoch 43/64\n",
            "156/156 [==============================] - 177s 1s/step - loss: 0.2144 - acc: 0.9583 - val_loss: 0.5517 - val_acc: 0.9111\n",
            "Epoch 44/64\n",
            "156/156 [==============================] - 173s 1s/step - loss: 0.2224 - acc: 0.9563 - val_loss: 0.5239 - val_acc: 0.9200\n",
            "Epoch 45/64\n",
            "156/156 [==============================] - 176s 1s/step - loss: 0.2483 - acc: 0.9491 - val_loss: 0.4122 - val_acc: 0.9387\n",
            "Epoch 46/64\n",
            "156/156 [==============================] - 181s 1s/step - loss: 0.2360 - acc: 0.9559 - val_loss: 0.6379 - val_acc: 0.8972\n",
            "Epoch 47/64\n",
            "156/156 [==============================] - 180s 1s/step - loss: 0.2258 - acc: 0.9521 - val_loss: 0.5555 - val_acc: 0.9032\n",
            "Epoch 48/64\n",
            "156/156 [==============================] - 176s 1s/step - loss: 0.2148 - acc: 0.9587 - val_loss: 0.5243 - val_acc: 0.9170\n",
            "Epoch 49/64\n",
            "156/156 [==============================] - 178s 1s/step - loss: 0.2170 - acc: 0.9585 - val_loss: 0.5983 - val_acc: 0.9130\n",
            "Epoch 50/64\n",
            "156/156 [==============================] - 179s 1s/step - loss: 0.2125 - acc: 0.9571 - val_loss: 0.5566 - val_acc: 0.9071\n",
            "Epoch 51/64\n",
            "156/156 [==============================] - 179s 1s/step - loss: 0.1831 - acc: 0.9675 - val_loss: 0.4969 - val_acc: 0.9051\n",
            "Epoch 52/64\n",
            "156/156 [==============================] - 177s 1s/step - loss: 0.1869 - acc: 0.9633 - val_loss: 0.4462 - val_acc: 0.9219\n",
            "Epoch 53/64\n",
            "156/156 [==============================] - 178s 1s/step - loss: 0.1701 - acc: 0.9710 - val_loss: 0.3809 - val_acc: 0.9269\n",
            "Epoch 54/64\n",
            "156/156 [==============================] - 178s 1s/step - loss: 0.1671 - acc: 0.9708 - val_loss: 0.4278 - val_acc: 0.9298\n",
            "Epoch 55/64\n",
            "156/156 [==============================] - 176s 1s/step - loss: 0.1753 - acc: 0.9690 - val_loss: 0.6166 - val_acc: 0.9140\n",
            "Epoch 56/64\n",
            "156/156 [==============================] - 175s 1s/step - loss: 0.2020 - acc: 0.9633 - val_loss: 0.4916 - val_acc: 0.9111\n",
            "Epoch 57/64\n",
            "156/156 [==============================] - 187s 1s/step - loss: 0.1980 - acc: 0.9605 - val_loss: 0.5507 - val_acc: 0.9170\n",
            "Epoch 58/64\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.2309 - acc: 0.9569 - val_loss: 0.5751 - val_acc: 0.9298\n",
            "Epoch 59/64\n",
            "156/156 [==============================] - 187s 1s/step - loss: 0.2841 - acc: 0.9531 - val_loss: 0.5435 - val_acc: 0.9239\n",
            "Epoch 60/64\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.2038 - acc: 0.9637 - val_loss: 0.5370 - val_acc: 0.9200\n",
            "Epoch 61/64\n",
            "156/156 [==============================] - 184s 1s/step - loss: 0.1793 - acc: 0.9679 - val_loss: 0.4559 - val_acc: 0.9289\n",
            "Epoch 62/64\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.1631 - acc: 0.9732 - val_loss: 0.4335 - val_acc: 0.9229\n",
            "Epoch 63/64\n",
            "156/156 [==============================] - 186s 1s/step - loss: 0.1813 - acc: 0.9667 - val_loss: 0.3950 - val_acc: 0.9269\n",
            "Epoch 64/64\n",
            "156/156 [==============================] - 187s 1s/step - loss: 0.1652 - acc: 0.9716 - val_loss: 0.5579 - val_acc: 0.9081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sXUucd8A-tkt",
        "colab_type": "code",
        "outputId": "8746c39c-c73d-4dcb-f899-43fda1ea4fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "train_datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'train_set',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_generator.class_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4992 images belonging to 14 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B.C. Rich': 0,\n",
              " 'Caparison': 1,\n",
              " 'DEAN': 2,\n",
              " 'ESP': 3,\n",
              " 'Fender': 4,\n",
              " 'Gibson': 5,\n",
              " 'Ibanez': 6,\n",
              " 'Jackson': 7,\n",
              " 'Kiesel': 8,\n",
              " 'Mayones': 9,\n",
              " 'Paul Reed Smith': 10,\n",
              " 'SCHECTER': 11,\n",
              " 'Strandberg': 12,\n",
              " 'Suhr': 13}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "K6uT3vyMyZbC",
        "colab_type": "code",
        "outputId": "d8b8f6fc-2ec7-485a-8950-8a76d52cad11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('model_1.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9ziJ7jaX_oQZ",
        "colab_type": "code",
        "outputId": "30c8c6b6-e7ca-40ca-bd67-aa3d9c315718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "img = Image.open('05-313457100_2.jpg')\n",
        "\n",
        "img_resize = img.resize((224, 224))\n",
        "\n",
        "img_array = np.asarray(img_resize)\n",
        "img_array = img_array / 255.\n",
        "img_array = img_array.reshape((1, 224, 224, 3))\n",
        "\n",
        "t1 = time.time()\n",
        "prediction = model.predict(img_array)\n",
        "print(np.argmax(prediction))\n",
        "t2 = time.time()\n",
        "print(t2 - t1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "1.637706995010376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VbSXrvmSDMBy",
        "colab_type": "code",
        "outputId": "efa8ae1d-e1eb-4b33-8df5-3974128a4520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.callbacks import EarlyStopping \n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NUM_TRAINING = 5005\n",
        "NUM_VALIDATION = 1046\n",
        "\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "base_model =MobileNet(weights='imagenet', include_top=False, \n",
        "                         input_tensor=input_tensor)\n",
        "\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "predictions = Dense(14, activation='softmax',kernel_regularizer=regularizers.l2(0.01), name='predictions')(x)\n",
        "model = Model(base_model.input, outputs=predictions)\n",
        "\n",
        "# for layer in model.layers[:72]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "#     # Batch Normalization の freeze解除\n",
        "#     if \"bn\" in layer.name:\n",
        "#         layer.trainable = True\n",
        "\n",
        "# #73層以降、学習させる\n",
        "# for layer in model.layers[72:]:\n",
        "#     layer.trainable = True\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'train_set',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "   'test_set',\n",
        "   target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "   batch_size=BATCH_SIZE,\n",
        "   class_mode='categorical',\n",
        "   shuffle=True\n",
        ")\n",
        "\n",
        "hist = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=NUM_TRAINING // BATCH_SIZE,\n",
        "    epochs=64,\n",
        "    verbose=1,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=NUM_VALIDATION//BATCH_SIZE\n",
        ")\n",
        "\n",
        "model.save('mobilenet_model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:208: UserWarning: MobileNet shape is undefined. Weights for input shape (224, 224) will be loaded.\n",
            "  warnings.warn('MobileNet shape is undefined.'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 4992 images belonging to 14 classes.\n",
            "Found 1044 images belonging to 14 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/64\n",
            "  1/156 [..............................] - ETA: 53:29 - loss: 5.5880 - acc: 0.0625"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:872: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 1734s 11s/step - loss: 14.6317 - acc: 0.0994 - val_loss: 14.7971 - val_acc: 0.0957\n",
            "Epoch 2/64\n",
            "156/156 [==============================] - 164s 1s/step - loss: 8.4381 - acc: 0.1072 - val_loss: 15.2344 - val_acc: 0.0741\n",
            "Epoch 3/64\n",
            "156/156 [==============================] - 167s 1s/step - loss: 3.3402 - acc: 0.1202 - val_loss: 9.7309 - val_acc: 0.0543\n",
            "Epoch 4/64\n",
            "156/156 [==============================] - 169s 1s/step - loss: 2.7459 - acc: 0.1583 - val_loss: 3.0314 - val_acc: 0.1136\n",
            "Epoch 5/64\n",
            "156/156 [==============================] - 167s 1s/step - loss: 2.6183 - acc: 0.1697 - val_loss: 2.8384 - val_acc: 0.1275\n",
            "Epoch 6/64\n",
            "156/156 [==============================] - 169s 1s/step - loss: 2.5302 - acc: 0.1833 - val_loss: 2.7674 - val_acc: 0.1225\n",
            "Epoch 7/64\n",
            "156/156 [==============================] - 168s 1s/step - loss: 2.4656 - acc: 0.1975 - val_loss: 2.8520 - val_acc: 0.1749\n",
            "Epoch 8/64\n",
            "156/156 [==============================] - 168s 1s/step - loss: 2.4069 - acc: 0.2214 - val_loss: 2.7316 - val_acc: 0.1660\n",
            "Epoch 9/64\n",
            "156/156 [==============================] - 166s 1s/step - loss: 2.3631 - acc: 0.2384 - val_loss: 3.6410 - val_acc: 0.1126\n",
            "Epoch 10/64\n",
            "156/156 [==============================] - 168s 1s/step - loss: 2.2829 - acc: 0.2514 - val_loss: 2.4681 - val_acc: 0.2352\n",
            "Epoch 11/64\n",
            "156/156 [==============================] - 167s 1s/step - loss: 2.2460 - acc: 0.2686 - val_loss: 3.3280 - val_acc: 0.2016\n",
            "Epoch 12/64\n",
            "156/156 [==============================] - 164s 1s/step - loss: 2.1827 - acc: 0.2975 - val_loss: 2.1169 - val_acc: 0.3300\n",
            "Epoch 13/64\n",
            "156/156 [==============================] - 168s 1s/step - loss: 2.1436 - acc: 0.3035 - val_loss: 2.3864 - val_acc: 0.3073\n",
            "Epoch 14/64\n",
            " 17/156 [==>...........................] - ETA: 1:26 - loss: 2.0769 - acc: 0.3713"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BgzqGzDUaUjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}